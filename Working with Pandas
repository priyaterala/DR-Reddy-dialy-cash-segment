#Creating and Manipulating Custom Dataframe
#created for training, validation, testing and predicting of
#Daily Share Value of 'DRREDDYs Lab' Cash Segment


import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib as plt
import io


#df dataframe consists of Dr.reddy's Share price values of Daily adjested close values,
#Weekly adjested close values and Monthly adjested close values from dates ranging 
#01-01-2018 to 15-06-2018.
#the value of date is converted to a float value


#reading the uploaded file from colab to the pandas dataframe df using read_csv
# and printing the dataframe

df = pd.read_csv(io.StringIO(uploaded['drdataset 2018.csv'].decode('utf-8')))
df

#O/p: 

Date	Monthly_Close	Weekly_Close	Daily_Close
0	43101	2237.250000	2447.649902	2404.149902
1	43102	2237.250000	2447.649902	2409.199951
2	43103	2237.250000	2447.649902	2339.000000
3	43104	2237.250000	2447.649902	2410.800049
4	43105	2237.250000	2447.649902	2473.399902
5	43108	2237.250000	2490.350098	2476.600098
6	43109	2237.250000	2490.350098	2457.800049
7	43110	2237.250000	2490.350098	2425.800049
8	43111	2237.250000	2490.350098	2425.350098
9	43112	2237.250000	2490.350098	2447.649902
10	43115	2237.250000	2507.750000	2431.750000
11	43116	2237.250000	2507.750000	2468.699951
12	43117	2237.250000	2507.750000	2491.850098
13	43118	2237.250000	2507.750000	2461.500000
14	43119	2237.250000	2507.750000	2490.350098
15	43122	2237.250000	2123.550049	2496.550049
16	43123	2237.250000	2123.550049	2520.399902
17	43124	2237.250000	2123.550049	2559.399902
18	43125	2237.250000	2123.550049	2507.750000
19	43129	2237.250000	2193.800049	2357.100098
20	43130	2237.250000	2193.800049	2306.149902
21	43131	2237.250000	2193.800049	2225.350098
22	43132	2080.550049	2193.800049	2157.949951
23	43133	2080.550049	2193.800049	2123.550049
24	43136	2080.550049	2213.550049	2116.500000
25	43137	2080.550049	2213.550049	2095.699951
26	43138	2080.550049	2213.550049	2111.399902
27	43139	2080.550049	2213.550049	2180.449951
28	43140	2080.550049	2213.550049	2193.800049
29	43143	2080.550049	2167.050049	2204.250000
...	...	...	...	...
84	43227	2269.399902	1982.000000	2069.500000
85	43228	2269.399902	1982.000000	2074.949951
86	43229	2269.399902	1982.000000	2064.350098
87	43230	2269.399902	1982.000000	1989.599976
88	43231	2269.399902	1982.000000	1990.500000
89	43234	2269.399902	1967.849976	2017.750000
90	43235	2269.399902	1967.849976	2008.500000
91	43236	2269.399902	1967.849976	1993.949951
92	43237	2269.399902	1967.849976	1977.150024
93	43238	2269.399902	1967.849976	1982.000000
94	43241	2269.399902	1941.400024	1893.050049
95	43242	2269.399902	1941.400024	2009.599976
96	43243	2269.399902	1941.400024	1954.349976
97	43244	2269.399902	1941.400024	1972.949951
98	43245	2269.399902	1941.400024	1967.849976
99	43248	2269.399902	2062.899902	1994.550049
100	43249	2269.399902	2062.899902	1980.699951
101	43250	2269.399902	2062.899902	1961.800049
102	43251	2269.399902	2062.899902	1936.599976
103	43252	2349.850098	2062.899902	1941.400024
104	43255	2349.850098	2349.850098	1996.000000
105	43256	2349.850098	2349.850098	1963.400024
106	43257	2349.850098	2349.850098	1960.449951
107	43258	2349.850098	2349.850098	1964.949951
108	43259	2349.850098	2349.850098	2062.899902
109	43262	2349.850098	2349.850098	2083.199951
110	43263	2349.850098	2349.850098	2190.649902
111	43264	2349.850098	2349.850098	2256.300049
112	43265	2349.850098	2349.850098	2269.399902
113	43266	2349.850098	2349.850098	2349.850098
114 rows Ã— 4 columns


#####################
#We get summary of the dataframe by using pandas describe()

df.describe()

#O/p:
       Date	Monthly_Close	Weekly_Close	Daily_Close
count	114.000000	114.000000	114.000000	114.000000
mean	43184.131579	2151.586414	2169.005709	2167.412715
std	49.342135	132.997153	158.943626	156.310001
min	43101.000000	1936.599976	1941.400024	1893.050049
25%	43140.750000	2080.550049	2074.362549	2081.212525
50%	43183.500000	2109.850098	2123.550049	2127.399902
75%	43227.750000	2269.399902	2231.500000	2211.225037
max	43266.000000	2349.850098	2507.750000	2559.399902
###############

#Displays top 5 records of dataframe

df.head()

#O/p:

  Date	Monthly_Close	Weekly_Close	Daily_Close
0	43101	2237.25	2447.649902	2404.149902
1	43102	2237.25	2447.649902	2409.199951
2	43103	2237.25	2447.649902	2339.000000
3	43104	2237.25	2447.649902	2410.800049
4	43105	2237.25	2447.649902	2473.399902
#############

#the first series(coloumn) of the dataframe is 'date', the date format is converted 
#to float values by using 'format cells' option in EXCEl 

df['Date'].head(10)

#o/p:
0    43101
1    43102
2    43103
3    43104
4    43105
5    43108
6    43109
7    43110
8    43111
9    43112
Name: Date, dtype: int64
########

df['Monthly_Close'].hist(bins=5)

#o/p:  Displays a histogram of 'Monthly_Close'
#########

df['Weekly_Close'].hist(bins=10)

#o/p:  Displays a histogram of 'Weekly_Close'
#########

df['Daily_Close'].hist(bins=20)

#o/p:  Displays a histogram of 'Daily_Close'
#########

#let us check the number of nulls / NaNs in the dataset
df.apply(lambda x: sum(x.isnull()),axis=0)

#o/p:
Date             0
Monthly_Close    0
Weekly_Close     0
Daily_Close      0
dtype: int64
############

#for converting 'Daily_close' values into logarithmic values 
#since few are at extremes, we make this so that the model learns better
#we use numpy log function
#and we display few updated records from dataframe

df['Daily_Close_log'] = np.log(df['Daily_Close'])
df.head()

#o/p:
Date	Monthly_Close	Weekly_Close	Daily_Close	Daily_Close_log
0	43101	2237.25	2447.649902	2404.149902	7.784952
1	43102	2237.25	2447.649902	2409.199951	7.787050
2	43103	2237.25	2447.649902	2339.000000	7.757479
3	43104	2237.25	2447.649902	2410.800049	7.787714
4	43105	2237.25	2447.649902	2473.399902	7.813349
###########

df['Daily_Close_log'].hist(bins=10)

#o/p:  Displays a histogram of 'Daily_Close_log'
#########
